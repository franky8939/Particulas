{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ENTRENAMIENTO DE TERSONFLOW PARA DATA DE ELECTROCARDIOGRAMAS**\n",
    "\n",
    "#NOTACION A USAR *son las variables q no son pertinentes en el estudio\n",
    "NOTA: Los datos originales poseen una fila que se debe eliminar a mano ya que la misma tiene mas columnas que las otras\n",
    "1. survival -- the number of months patient survived (has survived, if patient is still alive).  \n",
    "2. still-alive -- a binary variable.  0=dead at end of survival period, 1 means still alive \n",
    "3. age-at-heart-attack -- age in years when heart attack occurred\n",
    "4. pericardial-effusion -- binary. Pericardial effusion is fluid around the heart.  0=no fluid, 1=fluid\n",
    "5. fractional-shortening -- a measure of contracility around the heart lower numbers are increasingly abnormal\n",
    "6. epss -- E-point septal separation, another measure of contractility. Larger numbers are increasingly abnormal.\n",
    "7. lvdd -- left ventricular end-diastolic dimension.  This is a measure of the size of the heart at end-diastole. Large hearts tend to be sick hearts.\n",
    "8. wall-motion-score -- a measure of how the segments of the left ventricle are moving\n",
    "9. wall-motion-index -- equals wall-motion-score divided by number of segments seen.  Usually 12-13 segments are seen in an echocardiogram.  Use this variable INSTEAD of the wall motion score.\n",
    "10. mult -- a derivate var which can be ignored\n",
    "11. name -- the name of the patient (I have replaced them with \"name\")\n",
    "12. group -- meaningless, ignore it\n",
    "13. alive-at-1 -- Boolean-valued. Derived from the first two attributes. 0 means patient was either dead after 1 year or had been followed for less than 1 year.  1 means patient was alive at 1 year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PAQUETERIAS NECESARIAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow import keras\n",
    "# Bibliotecas de ayuda\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#Biblioteca para datos de entrenamiento\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ENTRADA DE LA INFORMACION, ORDENAMIENTO DE LA INFORMACION A USAR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTACION DE LAS VARIABLES \n",
    "label_total=[\"survival\", \"still_alive\", \"age-at-heart-attack\",\n",
    "       \"pericardial-effusion\",\"fractional-shortening\", \n",
    "       \"epss\", \"lvdd\",\"wall-motion-score\",\"wall-motion-index\",\n",
    "       \"mult\" , \"group\", \"alive-at-1\"];\n",
    "# ENTRAR LA INFORMACION DESDE UN ARCHIVO INTERNO\n",
    "data = pd.DataFrame(np.genfromtxt('echocardiogram.data', delimiter=','), columns= label_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FILTRADO DE LAS VARIABLES QUE SE UTILIZARAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_user=[\"age-at-heart-attack\", \"pericardial-effusion\",\"fractional-shortening\", \"epss\",\n",
    "            \"lvdd\",\"wall-motion-index\", \"alive-at-1\"]; # ELECCION DE LAS COLUMNAS\n",
    "select_data=data[label_user];#DATOS ORIGINALES SIN FILTRAR, COLUMNAS DE INTERES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONOCER EL OUTPUT**\n",
    "\n",
    "Este se relaciona con la posibilidad (en forma binaria) de encontrar al paciente vivo o muerto al aÃ±o del infarto. El formato es de dos columnas y las opciones permitidas son [1,0],[0,1],[-1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(select_data[\"alive-at-1-false\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\.conda\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "select_data[\"alive-at-1-false\"]=0;  #CREAR COLUMNA EXTRA\n",
    "for j in range(len(select_data)):   #SELECCION DE CONDICIONES\n",
    "    if   data[label_total[0]].values[j] > 12:   #VIVIO MAS DE 12 MESES\n",
    "         select_data[\"alive-at-1\"].values[j] = 1;       #CUMPLE EL REQUISITO\n",
    "         select_data[\"alive-at-1-false\"].values[j] = 0; \n",
    "    elif data[label_total[0]].values[j] == 12 and data[label_total[1]].values[j] == 1:  #CASO PARTICULAR, 12 MESES Y VIVE\n",
    "         select_data[\"alive-at-1\"].values[j] = 1;       #CUMPLE EL REQUISITO\n",
    "         select_data[\"alive-at-1-false\"].values[j] = 0; \n",
    "    elif data[label_total[0]].values[j] < 12 and data[label_total[1]].values[j] == 1:  #MENOS DE 12 MESES Y VIVE\n",
    "         select_data[\"alive-at-1\"].values[j] = -1;      #NO CUMPLE EL REQUISITO\n",
    "         select_data[\"alive-at-1-false\"].values[j] = -1;    \n",
    "    else:   # EL RESTO DE LOS DATOS\n",
    "         select_data[\"alive-at-1\"].values[j] = 0;       #CUMPLE EL REQUISITO DE ESTAR MUERTO\n",
    "         select_data[\"alive-at-1-false\"].values[j] = 1;\n",
    "#    if  random.random()>.5:\n",
    "#        select_data[\"alive-at-1\"].values[j] = 0; #ruido\n",
    "#        select_data[\"alive-at-1-false\"].values[j] = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select_data[\"alive-at-1-\"]=-1\n",
    "#print(select_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MATRIX LOGICA DE DATOS ELIMINAR LAS FILAS CON -1 (DESCONOCIDO)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(label_user)):\n",
    "    if i==0:\n",
    "        log=select_data[label_user[i]] !=-1;#print(i);\n",
    "    else:\n",
    "        log= np.logical_and(select_data[label_user[i]] !=-1,log)\n",
    "select_data=select_data[log]; # filtrar los desconocidos           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEFINICION DE MIS VALORES INPUT Y OUTPUT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data, input_data_test, output_data, output_data_test = train_test_split(\n",
    "    select_data[{\"age-at-heart-attack\", \"pericardial-effusion\",\"fractional-shortening\", \n",
    "                 \"epss\", \"lvdd\",\"wall-motion-index\"}], \n",
    "    select_data[{\"alive-at-1\",\"alive-at-1-false\"}], test_size=0.2, shuffle = True);\n",
    "    #select_data[{\"alive-at-1\"}], test_size=0.2, shuffle = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PARAMETROS INICIALES DE ENTRENAMIENTO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = np.int64(len(label_user)-1)   # input layer\n",
    "n_hidden1 = np.round(np.int64(len(label_user)/2))#np.int64(np.round(np.sqrt(len(label_user)))) # 1st hidden layer\n",
    "n_hidden2 = 256 # 2nd hidden layer\n",
    "n_output = np.int64(2)   # output \n",
    "\n",
    "learning_rate = 1e-4;   n_iterations = 1000   ;   batch_size = 3;         dropout = 0.5\n",
    "\n",
    "X = tf.compat.v1.placeholder(\"float\", [None, n_input]); Y = tf.compat.v1.placeholder(\"float\", [None, n_output])\n",
    "\n",
    "weights = {'w1': tf.Variable(tf.random.normal([n_input, n_hidden])),\n",
    "    'out': tf.Variable(tf.random.normal([n_hidden, n_output]))}\n",
    "\n",
    "biases = {'b1': tf.Variable(tf.constant(0.1, shape=[n_hidden])),\n",
    "    'out': tf.Variable(tf.constant(0.1, shape=[n_output]))}\n",
    "\n",
    "layer_1 = tf.add(tf.matmul(X, weights['w1']), biases['b1'])\n",
    "output_layer = tf.matmul(layer_1, weights['out']) + biases['out']\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=output_layer))\n",
    "train_step = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(output_layer, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INICIALIZAR EL ENTRENAMIENTO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "cost_summary = tf.compat.v1.summary.scalar(\"Cost\", cross_entropy)\n",
    "acc_summary = tf.compat.v1.summary.scalar(\"Accuracy\", accuracy)\n",
    "all_summary = tf.compat.v1.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(input_data.values,output_data.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ENTRENAMIENTO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 \t| Loss = 4.9490314 \t| Accuracy = 0.84615386\n",
      "Iteration 200 \t| Loss = 4.696648 \t| Accuracy = 0.84615386\n",
      "Iteration 400 \t| Loss = 4.4564056 \t| Accuracy = 0.84615386\n",
      "Iteration 600 \t| Loss = 4.2276564 \t| Accuracy = 0.84615386\n",
      "Iteration 800 \t| Loss = 4.009779 \t| Accuracy = 0.84615386\n",
      "\n",
      "Accuracy on test set: 0.8235294\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "0 1\n",
      "1 1\n",
      "1 1\n",
      "0 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "0 1\n",
      "1 0\n",
      "0 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "0 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n"
     ]
    }
   ],
   "source": [
    "#saver=tf.train.Saver() #SALVAR LA SECCION PARA PODERLA UTILIZAR LUEGO\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(\"Tensorboard\", sess.graph)\n",
    "    #print(sess)\n",
    "    sess.run(init)\n",
    "    #saver.save(sess, 'modelo')\n",
    "    for i in range(n_iterations):\n",
    "        sess.run(train_step, feed_dict={X: input_data.values, Y: output_data.values})\n",
    "        \n",
    "        summary_results, loss, acc = sess.run([all_summary, cross_entropy, accuracy], feed_dict={X: input_data.values, Y: output_data.values})\n",
    "        writer.add_summary(summary_results, i)\n",
    "        if i%200 == 0:\n",
    "            print(\"Iteration\", str(i), \"\\t| Loss =\", str(loss), \"\\t| Accuracy =\", str(acc))\n",
    "    \n",
    "        if acc==1:\n",
    "            break\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={X: input_data_test.values, Y: output_data_test.values})\n",
    "    print(\"\\nAccuracy on test set:\", test_accuracy)\n",
    "    \n",
    "    a, b = sess.run([tf.argmax(output_layer, 1),tf.argmax(Y,1)], feed_dict={X: input_data.values, Y: output_data.values}) \n",
    "    \n",
    "    for ea, eb in zip(a, b):\n",
    "        print(ea,eb)\n",
    "    #saver.save(sess, 'modelo') #SALVAR EL MODELO EXTERNAMENTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBAR EL MODELO CON LA DATA DE MUESTRA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390\n"
     ]
    }
   ],
   "source": [
    "print(input_data.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
